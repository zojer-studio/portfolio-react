---
title: Learning Unity [devlog.01]
subtitle: Feb 23, 204 • 3 min read
year: 2024
date: 2024-02-24
thumbnail: https://schultzdavidg-portfolio.s3.us-west-1.amazonaws.com/images/blog/devlog01/devlog01-hero.png
order: null
visible: true
---

<TitleSection />

![image](https://schultzdavidg-portfolio.s3.us-west-1.amazonaws.com/images/blog/devlog01/devlog01-hero.png)

Well, guess who just got a new Quest 3? I did, and I’ve spent the day inside it learning Unity!

Just because I have to, some quick impressions:
- The passthrough is pretty ass. I mean, it’s great, it does the job, but my expectations were set way too high by my demo in the AVP a few weeks ago.
- Controllers? Phenomenal. Hand tracking? Iffy.
- Quest Link — super smooth (besides some issues rendering my Unity client), but very disappointed I can’t use just my hands, keyboard, and mouse. It really breaks my flow when I need to pick up the controllers to move my windows around. Also hoping I can figure out passthrough…
- I’m surprised at how quickly I’ve adjusted to the motion sickness. It was real bad for the first 20min, but after a few hours (with it on + off my head) I think I’ve completely adjusted. Hoo-rah


Okay, so. I thought I’d start writing this devlog to track my journey into the mixed reality space. I’ve got a solid foundation in design, I’ve been programming things for years, and I’m increasingly excited about the possibilities of this space.

If I had to put it in a mission statement, I’d say my ultimate goal is to bring computing experiences back into the world around us. Our bodies have incredible capabilities, and we’ve (unfortunately) forgone them in favor of sitting around all day, staring at 2d screens. I want to play a part in this “spatial revolution” (or whichever grandiose term you’d like) by focusing on that somatic — bodily — connection between us and our interfaces.

p.s. VR takes users out of the world, but MR can bring them back into it. You could say I’m anti-escapism.

Anyhow. I’ve been in Unity today and it’s pretty intuitive so far! I’ve learned how to [create 2d UI screens](https://www.youtube.com/watch?v=mW8tRQRQD74), built a little [peashooter prototype](https://www.youtube.com/watch?v=OFIzdvnTiKU&list=PLPJUho5jpFX-Jecru96OkEh1SjLPMCJaI&index=3), used [sockets](https://www.google.com/search?sca_esv=39cb00dd45544bc1&rlz=1C1CHBF_enUS896US896&sxsrf=ACQVn09lf0TWqsYScV_H44P6MIvUhH9H4w:1708749843861&q=unity+sockets&tbm=vid&source=lnms&sa=X&ved=2ahUKEwj768-blcOEAxXFFTQIHdyKDQ0Q0pQJegQIDBAB&biw=1029&bih=766&dpr=1.25#fpstate=ive&vld=cid:3cf0b7a1,vid:rRNvq09Itdw,st:0) to anchor items, and am currently learning how to [create a passthrough experience with *real world anchors*](https://www.youtube.com/watch?v=nfffR3TLzzg&t=3s). Very few roadblocks today, and looking forward to building the calendar app my design team at UW has been crafting! Some progress pics below :)

Modeling in Blender is another story, and I don’t know if it’s my forte… but we’ll get to that on a later day. I think for now I’ll prioritize my Unity skillset, as it’s much more in line with what I’m actually interested in — interaction.

See ya!

![image](https://schultzdavidg-portfolio.s3.us-west-1.amazonaws.com/images/blog/devlog01/slide1.png)

![image](https://schultzdavidg-portfolio.s3.us-west-1.amazonaws.com/images/blog/devlog01/slide2.png)

![image](https://schultzdavidg-portfolio.s3.us-west-1.amazonaws.com/images/blog/devlog01/slide3.png)

![image](https://schultzdavidg-portfolio.s3.us-west-1.amazonaws.com/images/blog/devlog01/slide4.png)
