---
title: Thinking about constraints in XR
subtitle: Apr 16, 2024 • 4 min read
year: 2024
date: 2024-04-16
thumbnail: https://schultzdavidg-portfolio.s3.us-west-1.amazonaws.com/images/blog/constraints-in-xr/rumble-hero.jpg
order: null
visible: true
---

<TitleSection />

Daniel Marqusee (Lead Spatial Designer @ Bezi) just shared a nice [video](https://www.youtube.com/watch?v=6wAv1Ju4i0c&t=11s) about navigating constraints in XR, and I had some thoughts.

> So I guess, that’s my plan. To explore the deeper questions in XR [design], and help facilitate broader a conversation for creators and builders.

![image](https://schultzdavidg-portfolio.s3.us-west-1.amazonaws.com/images/blog/constraints-in-xr/thinking.png)
*Image by [Toptal](https://www.toptal.com/designers/ui/design-constraints)*

Lately, I’ve been thinking a lot about one constraint in particular—form factor. IMO, it’s the granddaddy of all of them. It is the fundamental basis for everything we design. And, while it’s constantly developing and changing, it will make or break our field. Form factor is our god.

At this moment in time (2024Q2), we have a few breakthrough consumer products, packed with pretty incredible technology. Compute is insanely fast, resolution is getting better, and input through hand + eyes isn’t just science-fiction anymore.

I’d argue that software is the bottleneck right now; [most apps are super gimmicky](https://www.youtube.com/watch?v=hH1IbNGgX6g), and “useful” products are still trying to establish their (clunky) design patterns. So the ball is in our court.

But in this post, I want to focus on the road ahead—namely, making this technology accessible to an everyday user.

#### Comfort

The most obvious (and least interesting) impact of form factor is on comfort. Adoption will be low so long as the discomfort outweighs its usefulness. But, it’s a bit out of our realm, as software designers. I think this topic comes into play as we shift from headsets to some other form.

<VideoPlayer
    width="100%"
    height="auto"
    videoUrl="https://schultzdavidg-portfolio.s3.us-west-1.amazonaws.com/images/blog/constraints-in-xr/rumble.mp4"
    gifUrl="https://schultzdavidg-portfolio.s3.us-west-1.amazonaws.com/images/blog/constraints-in-xr/rumble.gif"
/>

*I found this game Rumble, a gesture-based earthbending game that feels like a sport. It is *so* fun, and a great example of gestures done right.*

#### Input

Controllers, hands, eyes—which type of input is your product better suited towards? Do users need the macros from handheld controllers? Do they need the precision of a mouse + keyboard? Does your platform even have each type of input?
- For example, Rumble (see above) seems like the perfect game for pure hand-tracking. But, controllers provide some pretty important affordances: joystick movement, haptic feedback, push to talk, and more accurate positioning. Though it *sounds* cool to play with exclusively hand-tracking, I think the experience is better with controllers. *Side note: those haptics are extremely important to help users reinforce their muscle-memory.*
- Rumble also teaches us that gestural interactions have a steep, steep learning curve. To perform a move, your hand needs to be in a specific position + rotation. Even though you don’t need to be very precise, it’s surprisingly difficult to get it right consistently—and that makes new users feel like the game is broken. So there’s a high skill ceiling, but also a very high floor. Is that tradeoff worth it?
- As another example, in my capstone project (Quest3 + Unity), I’ve constrained myself entirely to hand-tracking, and have found that it’s difficult to perform precise tasks through this input. It gets tiring, to repetitively perform broad gestures. And, it’s hard to build muscle memory without physical reference points.


These are constraints/patterns that are being established *now*—but, something I’m looking forward to is Meta’s [wrist-worn EMG’s](https://www.youtube.com/watch?v=WmxLiXAo9ko). They’ll provide a much finer level of precision to gestural interaction, which I think will be a game changer for micro-level tasks.

But, it’s hard to get a sense of it + start defining those patterns ahead of the technology, without being able to test it w/ feedback.

*Also, it’s worth noting BCI’s (brain-computer interfaces) may be on the horizon. From what I gather though, for this input-method to be practical, you need something invasive—which (a) drives down accessibility, and (b) calls ethics into question.*

#### Social

The deeper conversation around form factor is on the social side of things.

It *sucks* to talk to someone with a headset on. And while Apple was on to something with AVP, I don’t think we’ll escape the uncanny valley unless everyone is able to access Meta’s Pixel Codec Avatars. *And even then, I don’t buy that people will prefer PiCA over real life.*

Ideally, (1) we can see each other’s faces, and (2) we can point + interact in a shared experience. We’ve got a good start on (2) in the context of VR (not yet MR), but figuring out (1) is going to be quite difficult.

One of the biggest problems with (1) lies with image display technology. If we move to a glasses form factor, visual fidelity suffers, majorly. A quick summary of the problem is that light is additive—so if you’re seeing the real world around you, your display needs to be brighter than your surroundings, or have someway of spot-occluding.

TechAltar has a great video on this: [https://youtu.be/VhFKKvKO6sU?si=FnjxJ9ei2Qp7pLVL](https://youtu.be/VhFKKvKO6sU?si=FnjxJ9ei2Qp7pLVL)

I get the sense that we’re not going to see many people using this technology outside of their homes until the form factor changes to something more sociable. And if that means, reducing the form factor to glasses, it will likely come with an entirely new paradigm—and major constraints—for us as designers.

There’s still so much to discuss and dive into! It’s an exciting time to be a designer. I don’t really know how to tie this post off. Hope this got your mind turning!
